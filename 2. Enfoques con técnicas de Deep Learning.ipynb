{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87814,"databundleVersionId":10024333,"sourceType":"competition"}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.experimental import enable_iterative_imputer  \nfrom sklearn.impute import SimpleImputer\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T04:39:49.281221Z","iopub.execute_input":"2024-12-22T04:39:49.281471Z","iopub.status.idle":"2024-12-22T04:39:54.316890Z","shell.execute_reply.started":"2024-12-22T04:39:49.281448Z","shell.execute_reply":"2024-12-22T04:39:54.315909Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/prediccion-de-sufrir-enfermedades-coronarias/train.csv')\ndf_test_public = pd.read_csv('/kaggle/input/prediccion-de-sufrir-enfermedades-coronarias/test_public.csv')\ndf_test_private = pd.read_csv('/kaggle/input/prediccion-de-sufrir-enfermedades-coronarias/test_private.csv')","metadata":{"execution":{"iopub.status.busy":"2024-12-22T04:39:54.317741Z","iopub.execute_input":"2024-12-22T04:39:54.319197Z","iopub.status.idle":"2024-12-22T04:39:56.155066Z","shell.execute_reply.started":"2024-12-22T04:39:54.319172Z","shell.execute_reply":"2024-12-22T04:39:56.154382Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"X,y = df.drop(['CHD_OR_MI','ID'],axis=1),df[['CHD_OR_MI']] \nx_train,x_val,y_train,y_val = train_test_split(X,y,test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T04:39:56.155850Z","iopub.execute_input":"2024-12-22T04:39:56.156083Z","iopub.status.idle":"2024-12-22T04:39:56.284624Z","shell.execute_reply.started":"2024-12-22T04:39:56.156065Z","shell.execute_reply":"2024-12-22T04:39:56.283945Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Otros enfoques utilizando técnicas de Deep Learning","metadata":{}},{"cell_type":"markdown","source":"Aparte de las técnicas de Boosting decidí utilizar otras técnicas de deep learning para intentar superar el f1_score alcanzado.","metadata":{}},{"cell_type":"markdown","source":"Lo que no funciono\n* Iterative imputer\n* Lasso and ridge logistic regression(empleados para clasificación)\n\nCon estas técncias no se superó el f1 score de 0.9\n\n\nLo que si funcionó (sin necesidad de aplicar over ni undersampling  ni oversampling)\n* Autoencoder (utilizando Keras) + LightGBM | F1_Score Test: 0.9576\n* TabNet (es un enfoque de deep learning para trabajar con data tabular) | F1_Score Test: 0.9578\n\nSe descartaron estos modelos por la pérdida de interpretabilidad y porque la implementación de TabNet en Pytorch es bastante limitada lo cual dificulta la optimización de hiperpárametros del modelo.","metadata":{}},{"cell_type":"markdown","source":"#### Autoencoder + Modelos de Boosting","metadata":{}},{"cell_type":"markdown","source":"Siguiendo una metodología similar a la propuesta por el paper: *A study on using deep autoencoders for imbalanced binary classification* decidí entrenar un *autoencoder* que aprenda una representación latente de la data. Luego recortar el autoencoder y usar ese vector caracteristico para entrenar 3 modelos de boosting. Con este enfoque no era necesario aplicar ni over ni undersampling","metadata":{}},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.regularizers import l1_l2\n\ninput_dim = x_train.shape[1]  \nencoding_dim = 64 \n\ninput_layer = Input(shape=(input_dim,))\nencoded = Dense(128, activation='relu')(input_layer)\nencoded = Dense(64, activation='relu')(encoded)\nbottleneck = Dense(encoding_dim, activation='relu')(encoded) # Esta capa  es la que más nos interesa porque luego hacemos el recorte aquí\ndecoded = Dense(64, activation='relu')(bottleneck)\ndecoded = Dense(128, activation='relu')(decoded)\noutput_layer = Dense(input_dim, activation='relu')(decoded)\nautoencoder = Model(inputs=input_layer, outputs=output_layer)\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')\nautoencoder.fit(x_train, x_train, epochs=50, batch_size=256, validation_split=0.2)\nencoder = Model(inputs=input_layer, outputs=bottleneck)\n\nx_latente_train = encoder.predict(x_train)\nx_latente_test = encoder.predict(x_val)","metadata":{"execution":{"iopub.status.busy":"2024-12-22T04:39:56.286504Z","iopub.execute_input":"2024-12-22T04:39:56.286834Z","iopub.status.idle":"2024-12-22T04:41:39.791866Z","shell.execute_reply.started":"2024-12-22T04:39:56.286801Z","shell.execute_reply":"2024-12-22T04:41:39.791088Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: nan - val_loss: nan\nEpoch 2/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 3/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 4/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 5/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 6/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 7/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 8/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 9/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 10/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 11/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 12/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 13/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 14/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 15/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 16/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 17/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 18/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 19/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 20/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 21/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 22/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 23/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 24/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 25/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 26/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 27/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 28/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 29/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 30/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 31/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 32/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 33/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 34/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 35/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 36/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 37/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 38/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 39/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 40/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 41/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 42/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 43/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 44/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 45/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 46/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 47/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 48/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 49/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\nEpoch 50/50\n\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - val_loss: nan\n\u001b[1m8682/8682\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step\n\u001b[1m2171/2171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def measure_performate_model_on_test(modelo,x_train,y_train,x_test,y_test):\n    modelo.fit(x_train,y_train)\n    y_pred = modelo.predict(x_test)\n    print(classification_report(y_test,y_pred))\n    print('F1 score')\n    print(f1_score(y_test,y_pred))\n\ncatboost_model = CatBoostClassifier(\n    scale_pos_weight=1, # 1\n    random_state=42,\n    verbose=False)\n\nxgboost_model = XGBClassifier(\n    scale_pos_weight=1, \n    random_state=42,\n    )  \n\nlightgbm_model = LGBMClassifier(\n    scale_pos_weight=1,   \n    random_state=42,\n    verbose=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T04:41:39.793487Z","iopub.execute_input":"2024-12-22T04:41:39.794177Z","iopub.status.idle":"2024-12-22T04:41:39.801708Z","shell.execute_reply.started":"2024-12-22T04:41:39.794151Z","shell.execute_reply":"2024-12-22T04:41:39.800731Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print('Resultados del catboost con hiperparametros default + Autoencoder')\nmeasure_performate_model_on_test(catboost_model,x_latente_train,y_train,x_latente_test,y_val) \nprint('Resultados del xgboost con hiperparametros default + Autoencoder')\nmeasure_performate_model_on_test(xgboost_model,x_latente_train,y_train,x_latente_test,y_val)\nprint('Resultados del light_gbm con hiperparametros default + Autoencoder')\nmeasure_performate_model_on_test(lightgbm_model,x_latente_train,y_train,x_latente_test,y_val)","metadata":{"execution":{"iopub.status.busy":"2024-12-22T04:41:39.802726Z","iopub.execute_input":"2024-12-22T04:41:39.803046Z","iopub.status.idle":"2024-12-22T04:41:54.290536Z","shell.execute_reply.started":"2024-12-22T04:41:39.803012Z","shell.execute_reply":"2024-12-22T04:41:54.289542Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Resultados del catboost con hiperparametros default + Autoencoder\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         0.0       0.00      0.00      0.00      5713\n         1.0       0.92      1.00      0.96     63737\n\n    accuracy                           0.92     69450\n   macro avg       0.46      0.50      0.48     69450\nweighted avg       0.84      0.92      0.88     69450\n\nF1 score\n0.9571054232019641\nResultados del xgboost con hiperparametros default + Autoencoder\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         0.0       0.00      0.00      0.00      5713\n         1.0       0.92      1.00      0.96     63737\n\n    accuracy                           0.92     69450\n   macro avg       0.46      0.50      0.48     69450\nweighted avg       0.84      0.92      0.88     69450\n\nF1 score\n0.9571054232019641\nResultados del light_gbm con hiperparametros default + Autoencoder\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n              precision    recall  f1-score   support\n\n         0.0       0.00      0.00      0.00      5713\n         1.0       0.92      1.00      0.96     63737\n\n    accuracy                           0.92     69450\n   macro avg       0.46      0.50      0.48     69450\nweighted avg       0.84      0.92      0.88     69450\n\nF1 score\n0.9571054232019641\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"#### TabNet con modificación de la función de pérdida ","metadata":{}},{"cell_type":"code","source":"!pip install pytorch_tabnet\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom sklearn.metrics import f1_score\nfrom pytorch_tabnet.metrics import Metric\nimport numpy as np\nimport torch\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-12-22T04:42:16.233800Z","iopub.execute_input":"2024-12-22T04:42:16.234168Z","iopub.status.idle":"2024-12-22T04:42:23.973635Z","shell.execute_reply.started":"2024-12-22T04:42:16.234136Z","shell.execute_reply":"2024-12-22T04:42:23.972817Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting pytorch_tabnet\n  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.26.4)\nRequirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.2.2)\nRequirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.13.1)\nRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (2.4.1+cu121)\nRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (4.66.5)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch_tabnet) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch_tabnet) (1.3.0)\nDownloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pytorch_tabnet\nSuccessfully installed pytorch_tabnet-4.1.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"En el paper *Clash of titans on imbalanced data: TabNet vs \nXGBoost (se encuentra en las referencias) proponen 3 modificaciones a la función de pérdida de TabNet*","metadata":{}},{"cell_type":"markdown","source":"Se implementó la Loss Categorical Cross Entropy con una Mean divergence regularization (MDR). Esta debería penalizar al modelo para que se enfoque en la clase más subrepresentada tambien.","metadata":{}},{"cell_type":"markdown","source":"![image.png](attachment:6f266bb8-b648-4386-ad86-073db2aa8b12.png)","metadata":{},"attachments":{"6f266bb8-b648-4386-ad86-073db2aa8b12.png":{"image/png":"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"}}},{"cell_type":"code","source":"# PyTorch no tiene una metrica de F1score tuve que crear una clase que hereda métodos de la clase Metric de PyTorch\nclass F1Score(Metric):\n    def __init__(self):\n        self._name = \"f1_score\"\n        self._maximize = True\n    def __call__(self, y_true, y_score):\n        y_pred = np.argmax(y_score, axis=1)\n        return f1_score(y_true, y_pred, average=\"binary\")","metadata":{"execution":{"iopub.status.busy":"2024-12-22T04:42:23.974776Z","iopub.execute_input":"2024-12-22T04:42:23.975065Z","iopub.status.idle":"2024-12-22T04:42:23.979374Z","shell.execute_reply.started":"2024-12-22T04:42:23.975032Z","shell.execute_reply":"2024-12-22T04:42:23.978508Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class CE_MDR_Loss(torch.nn.Module):\n    def __init__(self, lambda_reg=0.1):\n        super(CE_MDR_Loss, self).__init__()\n        self.lambda_reg = lambda_reg # este lambda es un hiperparametro de la fórmula de arriba que determina el \"peso\" de la regularización \n    def forward(self, y_pred, y_true):\n        ce = F.cross_entropy(y_pred, y_true)\n        mean_pred = torch.mean(torch.softmax(y_pred, dim=1), dim=0) # Se aplica una softmax al vector de salida del modelo (eran un vector con las probabilidades de pertenecer a cada clase)\n        mdr_loss = torch.norm(mean_pred - 0.5, p=2)**2 / 2\n        perdida_total = ce + self.lambda_reg * mdr_loss\n        return perdida_total\npersonal_loss_function = CE_MDR_Loss(lambda_reg=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-12-22T04:42:24.263215Z","iopub.execute_input":"2024-12-22T04:42:24.263542Z","iopub.status.idle":"2024-12-22T04:42:24.268751Z","shell.execute_reply.started":"2024-12-22T04:42:24.263511Z","shell.execute_reply":"2024-12-22T04:42:24.267854Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def fit_transform_imputer(imputer,x_train,x_test):\n    x_train_imputed = imputer.fit_transform(x_train)\n    x_test_imputed = imputer.transform(x_test)\n    return x_train_imputed,x_test_imputed\n\n# TabNet solo funciona con imputacion\niterativ_imputer = SimpleImputer(strategy='most_frequent') #usamos la moda porque la mayor parte de las variables son categoricas\nx_train_itev,x_val_itev = fit_transform_imputer(iterativ_imputer,x_train,x_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T04:45:04.688800Z","iopub.execute_input":"2024-12-22T04:45:04.689185Z","iopub.status.idle":"2024-12-22T04:45:05.169768Z","shell.execute_reply.started":"2024-12-22T04:45:04.689150Z","shell.execute_reply":"2024-12-22T04:45:05.168841Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model = TabNetClassifier(optimizer_params=dict(lr=0.02), verbose=1)\nmodel.fit(\n    x_train_itev, y_train.to_numpy().reshape(-1),\n    eval_set=[(x_val_itev,y_val.to_numpy().reshape(-1))],\n    eval_name=[\"test\"],\n    eval_metric=[F1Score], # Utilizamos la métrica\n    loss_fn=personal_loss_function, # y la función de pérdida personalizada creadas anteriormente\n    max_epochs=20,\n    patience=3,\n    batch_size=256,\n    virtual_batch_size=128)","metadata":{"execution":{"iopub.status.busy":"2024-12-22T04:45:25.229581Z","iopub.execute_input":"2024-12-22T04:45:25.229879Z"},"trusted":true},"outputs":[{"name":"stdout","text":"epoch 0  | loss: 0.26134 | test_f1_score: 0.95711 |  0:00:29s\nepoch 1  | loss: 0.24082 | test_f1_score: 0.95711 |  0:00:59s\nepoch 2  | loss: 0.23888 | test_f1_score: 0.95652 |  0:01:28s\nepoch 3  | loss: 0.23765 | test_f1_score: 0.95711 |  0:01:57s\n\nEarly stopping occurred at epoch 3 with best_epoch = 0 and best_test_f1_score = 0.95711\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Referencias","metadata":{}},{"cell_type":"markdown","source":"* Tomescu, V.-I., Czibula, G., & Niţică, Ş. (2021). A study on using deep autoencoders for imbalanced binary classification. Procedia Computer Science, 192, 119-128. https://doi.org/10.1016/j.procs.2021.08.013.\n\n* Kanasz, R., Drotar, P., Gnip, P., & Zoricak, M. (2024). Clash of titans on imbalanced data: TabNet vs XGBoost. 2024 IEEE Conference on Artificial Intelligence (CAI), 320–325. https://doi.org/10.1109/CAI59869.2024.00068","metadata":{}}]}